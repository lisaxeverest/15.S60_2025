{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #6: Backtracking line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline: January 30, 2025"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this assignment, you will be implementing backtracking line search for the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function). Backtracking line search is a heuristic approach to compute the step size in gradient descent. \n",
    "\n",
    "The assignment is organized into 3 parts to help guide your implementation:\n",
    "- Step 1: Algorithm (fill in the TO DOs)\n",
    "- Step 2: Visualize (run the cells)\n",
    "- Step 3: Analyze (write a few sentences discussing what you found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, CSV, DataFrames, Random, NLsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Algorithm\n",
    "\n",
    "Backtracking line search requires two parameters: $\\sigma$ and $\\beta$. It starts with unit step size $\\alpha = 1$ and then reduces it by the factor $\\beta$. Let's break down the method below:\n",
    "\n",
    "1. Given a descent direction $d$ for $f$ at $x \\in \\textbf{dom} f$, $\\sigma \\in (0, 0.5], \\beta \\in (0, 1)$.\n",
    "\n",
    "2. Set $\\alpha = 1$. \n",
    "\n",
    "3. While $f(x + \\alpha d) > f(x) + \\sigma \\alpha \\nabla f(x)^Tâ€€d$, then let $\\alpha = \\beta \\alpha$.\n",
    "\n",
    "In gradient descent, the search direction $d$ is the negative of the gradient, so $d = - \\nabla f(x)$. We can rewrite the condition in step 3 as:\n",
    "\n",
    "$$f(x - \\alpha \\nabla f(x)) > f(x) - \\sigma \\alpha || \\nabla f(x) ||^2$$\n",
    "\n",
    "\n",
    "Complete the TO DOs below to implement gradient descent with backtracking line search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_rosenbrock (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cost_function_rosenbrock(x)\n",
    "    return 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2\n",
    "end\n",
    "\n",
    "function gradient_rosenbrock(x)\n",
    "    return [\n",
    "        - 400 * x[1] * (x[2] - x[1]^2) - 2*(1 - x[1]),\n",
    "          200 * (x[2] - x[1]^2),\n",
    "    ]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient_descent_backtracking_linesearch(\n",
    "    cost_function::Function,\n",
    "    gradient::Function,\n",
    "    initial_x::Vector,  # Initial point\n",
    "    epsilon,            # Termination parameter\n",
    "    s,                  # Initial alpha\n",
    "    sigma,              # Backtracking parameter in (0, 0.5]\n",
    "    beta,               # Rate of decrease in alpha in (0, 1)\n",
    ")\n",
    "    # Initialization\n",
    "    x = initial_x\n",
    "    k = 0\n",
    "    x_history = zeros(Float64, (0, 2))\n",
    "    cost_history = Float64[]\n",
    "    gradient_norm_history = Float64[]\n",
    "    alpha_history = Float64[]\n",
    "\n",
    "    while true\n",
    "        # Find descent direction d\n",
    "        gradient_val = gradient(x)\n",
    "        cost_function_val = cost_function(x)\n",
    "        gradient_norm = norm(gradient_val)\n",
    "        d = #TO DO\n",
    "\n",
    "        # Compute step size alpha\n",
    "        alpha = s\n",
    "        while #TO DO\n",
    "            alpha = #TO DO: update alpha\n",
    "        end\n",
    "\n",
    "        # Update history\n",
    "        x_history = vcat(x_history, x')\n",
    "        push!(cost_history, cost_function_val)\n",
    "        push!(gradient_norm_history, gradient_norm)\n",
    "        push!(alpha_history, alpha)\n",
    "\n",
    "        if gradient_norm < epsilon\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # Update x\n",
    "        x = # TO DO\n",
    "\n",
    "        # Increment iteration count\n",
    "        k += 1\n",
    "    end\n",
    "    return Dict(\n",
    "        \"x\" => x_history,\n",
    "        \"cost\" => cost_history,\n",
    "        \"gradient_norm\" => gradient_norm_history,\n",
    "        \"alpha\" => alpha_history,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize\n",
    "\n",
    "Run the following code to visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x_rosenbrock = [0.25, 4.5]\n",
    "epsilon = 1e-5;\n",
    "\n",
    "results_rosenbrock_backtracking_linesearch = @time gradient_descent_backtracking_linesearch(\n",
    "    cost_function_rosenbrock,\n",
    "    gradient_rosenbrock,\n",
    "    initial_x_rosenbrock,\n",
    "    epsilon,\n",
    "    2,      # s\n",
    "    0.25,   # sigma\n",
    "    0.5,    # beta\n",
    ");\n",
    "\n",
    "length(results_rosenbrock_backtracking_linesearch[\"cost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of x and y values\n",
    "x_range = 0:0.01:3\n",
    "y_range = 0:0.01:5\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "z = [cost_function_rosenbrock([x y]) for (x, y) in grid]\n",
    "z = reshape(z, length(x_range), length(y_range))'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_rosenbrock_backtracking_linesearch = contour(\n",
    "    x_range, y_range, z, \n",
    "    levels = 1000, \n",
    "    c = :viridis, color = :auto, \n",
    "    legend = false,\n",
    ")\n",
    "Plots.plot!(\n",
    "    results_rosenbrock_backtracking_linesearch[\"x\"][:,1],\n",
    "    results_rosenbrock_backtracking_linesearch[\"x\"][:,2],\n",
    "    linestyle = :dash,\n",
    "    linewidth = 2,\n",
    "    markershape = :circle, \n",
    "    color = :red,\n",
    "    title = \"Gradient descent with backtracking_linesearch step size ($(length(results_rosenbrock_backtracking_linesearch[\"cost\"])) iterations)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze\n",
    "\n",
    "For completion, answer the following questions briefly:\n",
    "\n",
    "- How does backtracking compare with constant step size gradient descent (see the the lecture notebook)? In particular, compare the number of iterations for the Rosenbrock function and discuss convergence (at a high level).\n",
    "\n",
    "ANSWER:\n",
    "\n",
    "- How does backtracking line search compare with exact line search? You can answer this qualitatively by addressing the computational cost and number of iterations. \n",
    "\n",
    "ANSWER: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a PDF version of this notebook to Canvas (with steps 1 and 3 completed) by January 30 at 11:59 p.m. Please reach out to Shriya (karam809@mit.edu) if you have any questions!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
